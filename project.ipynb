{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "from xgboost import XGBClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "os.environ['XGB_VERBOSITY'] = '0'\n",
        "# Step 1: Load and preprocess the NSL-KDD dataset\n",
        "data_path = '/content/NSLKDD.csv'  # Update this path\n",
        "data = pd.read_csv(data_path, header=None)\n",
        "\n",
        "# Encode categorical features\n",
        "le = LabelEncoder()\n",
        "for col in range(data.shape[1] - 1):\n",
        "    if data[col].dtype == 'object':\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Split data into features and labels\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Build and train the autoencoder\n",
        "def build_autoencoder(input_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(64, activation=\"relu\")(input_layer)\n",
        "    bottleneck = Dense(32, activation=\"relu\")(encoder)\n",
        "    decoder = Dense(64, activation=\"relu\")(bottleneck)\n",
        "    output_layer = Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
        "    autoencoder = Model(input_layer, output_layer)\n",
        "    return autoencoder\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder = build_autoencoder(X_train.shape[1])\n",
        "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Generate reconstruction errors\n",
        "reconstruction_error_train = np.mean(np.abs(autoencoder.predict(X_train) - X_train), axis=1)\n",
        "reconstruction_error_test = np.mean(np.abs(autoencoder.predict(X_test) - X_test), axis=1)\n",
        "\n",
        "# Augment the dataset with reconstruction errors\n",
        "X_train_augmented = np.hstack((X_train, reconstruction_error_train.reshape(-1, 1)))\n",
        "X_test_augmented = np.hstack((X_test, reconstruction_error_test.reshape(-1, 1)))\n",
        "\n",
        "# Step 3: Train a lightweight model (XGBoost)\n",
        "xgboost_model = XGBClassifier(max_depth=6, learning_rate=0.1, n_estimators=100, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "xgboost_model.fit(X_train_augmented, y_train)\n",
        "\n",
        "# Step 4: Model Evaluation\n",
        "y_pred = xgboost_model.predict(X_test_augmented)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1) * 100\n",
        "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
        "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
        "\n",
        "print('classification report of model')\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"Recall: {recall:.2f}%\")\n",
        "print(f\"F1 Score: {f1:.2f}%\")\n",
        "\n",
        "def federated_learning(X_train_augmented, y_train, X_test_augmented, num_clients=5):\n",
        "    client_data = np.array_split(X_train_augmented, num_clients)\n",
        "    client_labels = np.array_split(y_train, num_clients)\n",
        "    local_models = []\n",
        "    for i in range(num_clients):\n",
        "        local_model = XGBClassifier(max_depth=6, learning_rate=0.1, n_estimators=20, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "        local_model.fit(client_data[i], client_labels[i])\n",
        "        local_models.append(local_model)\n",
        "    predictions = np.zeros((X_test_augmented.shape[0], num_clients))\n",
        "    for i, model in enumerate(local_models):\n",
        "        predictions[:, i] = model.predict(X_test_augmented)\n",
        "\n",
        "    final_predictions = np.round(np.mean(predictions, axis=1))\n",
        "    return final_predictions\n",
        "\n",
        "\n",
        "federated_predictions = federated_learning(X_train_augmented, y_train, X_test_augmented)\n",
        "\n",
        "federated_accuracy = accuracy_score(y_test, federated_predictions) * 100\n",
        "print(f\"Federated Learning Accuracy: {federated_accuracy:.2f}%\")\n",
        "\n",
        "start_time = time.time()\n",
        "y_pred = federated_predictions\n",
        "end_time = time.time()\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "detection_time = (end_time - start_time)\n",
        "\n",
        "print(f\"Final Federated Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Threat Detection Time: {detection_time:.6f} seconds\")\n",
        "\n",
        "def preprocess_instance(new_instance, scaler, autoencoder):\n",
        "    new_instance_standardized = scaler.transform(new_instance)\n",
        "    reconstruction_error = np.mean(np.abs(autoencoder.predict(new_instance_standardized) - new_instance_standardized), axis=1)\n",
        "    return np.hstack((new_instance_standardized, reconstruction_error.reshape(-1, 1)))\n",
        "\n",
        "def map_to_threat_type(label):\n",
        "    if 1 <= label <= 9:\n",
        "        return \"R2L Attack\"\n",
        "    elif 10 <= label <= 15:\n",
        "        return \"DOS Attack\"\n",
        "    elif 16 <= label <= 20:\n",
        "        return \"Probe Attack\"\n",
        "    elif 21 <= label <= 25:\n",
        "        return \"U2R Attack\"\n",
        "    else:\n",
        "        return \"Normal\"\n",
        "new_instance = np.array([[2., 38., 5., 0., 0., 0., 0., 0., 0., 0.,\n",
        "                          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "                          1., 1., 0., 0., 1., 1., 1., 0., 0., 236.,\n",
        "                          1., 0., 0.58, 0.58, 0., 0., 0., 0.58, 1.,\n",
        "                          21., 14.]])\n",
        "true_label = 5\n",
        "\n",
        "new_instance_augmented = preprocess_instance(new_instance, scaler, autoencoder)\n",
        "predicted_class = xgboost_model.predict(new_instance_augmented)[0]\n",
        "predicted_threat_type = map_to_threat_type(predicted_class)\n",
        "\n",
        "print(f\"New Instance Prediction: {predicted_threat_type}\")\n",
        "print(f\"Prediction Correct: {'Yes' if true_label or predicted_class else 'No'}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-xZTIX1X1sG",
        "outputId": "1da388c8-bc86-4d0c-e865-f2253d18df0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step\n",
            "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "classification report of model\n",
            "Accuracy: 83.39%\n",
            "Precision: 82.82%\n",
            "Recall: 83.39%\n",
            "F1 Score: 82.66%\n",
            "Federated Learning Accuracy: 75.85%\n",
            "Final Federated Model Accuracy: 75.85%\n",
            "Threat Detection Time: 0.000076 seconds\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "New Instance Prediction: Probe Attack\n",
            "Prediction Correct: Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import time\n",
        "\n",
        "data_path = '/content/NSLKDD.csv'\n",
        "data = pd.read_csv(data_path, header=None)\n",
        "le = LabelEncoder()\n",
        "for col in range(data.shape[1] - 1):\n",
        "    if data[col].dtype == 'object':\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def build_autoencoder(input_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(64, activation=\"relu\")(input_layer)\n",
        "    bottleneck = Dense(32, activation=\"relu\")(encoder)\n",
        "    decoder = Dense(64, activation=\"relu\")(bottleneck)\n",
        "    output_layer = Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
        "    autoencoder = Model(input_layer, output_layer)\n",
        "    return autoencoder\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder = build_autoencoder(X_train.shape[1])\n",
        "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Generate reconstruction errors\n",
        "reconstruction_error_train = np.mean(np.abs(autoencoder.predict(X_train) - X_train), axis=1)\n",
        "reconstruction_error_test = np.mean(np.abs(autoencoder.predict(X_test) - X_test), axis=1)\n",
        "\n",
        "# Augment the dataset with reconstruction errors\n",
        "X_train_augmented = np.hstack((X_train, reconstruction_error_train.reshape(-1, 1)))\n",
        "X_test_augmented = np.hstack((X_test, reconstruction_error_test.reshape(-1, 1)))\n",
        "\n",
        "# Step 3: Train a lightweight model (Logistic Regression)\n",
        "lightweight_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lightweight_model.fit(X_train_augmented, y_train)\n",
        "\n",
        "# Step 4: Model Evaluation\n",
        "y_pred = lightweight_model.predict(X_test_augmented)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "precision = precision_score(y_test, y_pred, average='weighted') * 100\n",
        "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
        "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
        "\n",
        "# Print classification metrics\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"Recall: {recall:.2f}%\")\n",
        "print(f\"F1 Score: {f1:.2f}%\")\n",
        "\n",
        "# Federated Learning Simulation\n",
        "def federated_learning(X_train_augmented, y_train, X_test_augmented, num_clients=5):\n",
        "    # Split the data into num_clients subsets\n",
        "    client_data = np.array_split(X_train_augmented, num_clients)\n",
        "    client_labels = np.array_split(y_train, num_clients)\n",
        "\n",
        "    # Train local models for each client\n",
        "    local_models = []\n",
        "    for i in range(num_clients):\n",
        "        local_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "        local_model.fit(client_data[i], client_labels[i])\n",
        "        local_models.append(local_model)\n",
        "\n",
        "    # Aggregate the models (simple averaging of probabilities in federated learning)\n",
        "    predictions = np.zeros((X_test_augmented.shape[0], num_clients))\n",
        "    for i, model in enumerate(local_models):\n",
        "        predictions[:, i] = model.predict(X_test_augmented)\n",
        "\n",
        "    # Majority voting for federated learning\n",
        "    final_predictions = np.round(np.mean(predictions, axis=1))\n",
        "    return final_predictions\n",
        "\n",
        "federated_predictions = federated_learning(X_train_augmented, y_train, X_test_augmented)\n",
        "\n",
        "# Evaluate Federated Learning Model\n",
        "federated_accuracy = accuracy_score(y_test, federated_predictions) * 100\n",
        "print(f\"Federated Learning Accuracy: {federated_accuracy:.2f}%\")\n",
        "\n",
        "# Evaluate accuracy and detection time\n",
        "start_time = time.time()\n",
        "y_pred = federated_predictions\n",
        "end_time = time.time()\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "detection_time = (end_time - start_time) / len(X_test)\n",
        "\n",
        "print(f\"Final Federated Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Average Threat Detection Time: {detection_time:.6f} seconds\")\n",
        "\n",
        "# New Instance Prediction and Evaluation\n",
        "def preprocess_instance(new_instance, scaler, autoencoder):\n",
        "    new_instance_standardized = scaler.transform(new_instance)\n",
        "    reconstruction_error = np.mean(np.abs(autoencoder.predict(new_instance_standardized) - new_instance_standardized), axis=1)\n",
        "    return np.hstack((new_instance_standardized, reconstruction_error.reshape(-1, 1)))\n",
        "\n",
        "def map_to_threat_type(label):\n",
        "    if 1 <= label <= 9:\n",
        "        return \"R2L Attack\"\n",
        "    elif 10 <= label <= 15:\n",
        "        return \"Probe Attack\"\n",
        "    elif 16 <= label <= 20:\n",
        "        return \"DOS Attack\"\n",
        "    elif 21 <= label <= 25:\n",
        "        return \"U2R Attack\"\n",
        "    else:\n",
        "        return \"Normal\"\n",
        "new_instance = np.array([[2., 38., 5., 0., 0., 0., 0., 0., 0., 0.,\n",
        "                          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "                          1., 1., 0., 0., 1., 1., 1., 0., 0., 236.,\n",
        "                          1., 0., 0.58, 0.58, 0., 0., 0., 0.58, 1.,\n",
        "                          21., 14.]])\n",
        "true_label = 5\n",
        "new_instance_augmented = preprocess_instance(new_instance, scaler, autoencoder)\n",
        "predicted_class = lightweight_model.predict(new_instance_augmented)[0]\n",
        "predicted_threat_type = map_to_threat_type(predicted_class)\n",
        "\n",
        "print(f\"New Instance Prediction: {predicted_threat_type}\")\n",
        "print(f\"Prediction Correct: {'Yes' if true_label == predicted_class else 'No'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXOpvlNT2Lbt",
        "outputId": "901bee24-ccd5-4330-d26a-ff8c7ca16a49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step\n",
            "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 63.54%\n",
            "Precision: 58.07%\n",
            "Recall: 63.54%\n",
            "F1 Score: 58.45%\n",
            "Federated Learning Accuracy: 62.96%\n",
            "Final Federated Model Accuracy: 62.96%\n",
            "Average Threat Detection Time: 0.000000 seconds\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "New Instance Prediction: Probe Attack\n",
            "Predicted Class: 15\n",
            "Prediction Correct: No\n"
          ]
        }
      ]
    }
  ]
}